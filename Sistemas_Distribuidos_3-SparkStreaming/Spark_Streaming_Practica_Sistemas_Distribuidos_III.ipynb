{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Práctica Sistemas Distribuidos III \n",
    "## Spark Streaming. Quatar GP 2014"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilizando como base las herramientas presentadas en clase (productor y consumidor de Kafka\n",
    "genéricos en Python), crear una aplicación local de Spark Streaming que lea progresivamente los\n",
    "tweets insertados en una cola de Kafka identificada por el topic \"Quatar_GP_2014\", defina un\n",
    "intervalo de procesamiento de datos de 5 segundos y realice las siguientes tareas:\n",
    "\n",
    "a) Calcular el número total de menciones recibidas por cada cuenta de usuario durante el intervalo de 5 segundos.\n",
    "\n",
    "b) Calcular la frecuencia total acumulada de apariciones de cada hashtag en el campo body, actualizando un ranking con los 5 hashtags con mayor frecuencia de aparición.\n",
    "\n",
    "c) Calcular en una ventana temporal 20 segundos con offset de 10 segundos la frecuencia de aparición de cada uno de los 3 posibles tipos de tweets (TW-RT-MT)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importamos Librerias y creamos el contexto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importación de dependencias y funciones\n",
    "from __future__ import print_function\n",
    "from pyspark import SparkContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.streaming.kafka import KafkaUtils\n",
    "from operator import add\n",
    "from operator import sub\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load external packages programatically\n",
    "import os\n",
    "packages = \"org.apache.spark:spark-streaming-kafka-0-8_2.11:2.2.1\"\n",
    "\n",
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"] = (\n",
    "    \"--packages {0} pyspark-shell\".format(packages))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = SparkContext(appName=\"Quatar_GP_2014\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear el contexto de Spark Streaming\n",
    "ssc = StreamingContext(sc, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Métodos necesarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def parse_tweet(line):\n",
    "  s = re.split(r\",(?=(?:[^\\\"]*\\\"[^\\\"]*\\\")*[^\\\"]*$)\",line) # Usamos expresion regular para no dividir por , dentro de \"\"\n",
    "  try:\n",
    "      return [{\"Id\": int(s[0]), \"Parent_sys_id\": s[1], \"Source\": s[2], \n",
    "               \"Mentions\": s[3], \"Target\": s[4], \"Name_source\": s[5], \n",
    "               \"Body\": s[6], \"Pub_date\": datetime.strptime(s[7], \"%d/%m/%Y %H:%M\"), \n",
    "               \"URLs\": s[8], \"Tipe_action\": s[9], \"Link\": s[10],\"Has_link\": s[11],\n",
    "               \"Has_picture\": s[12], \"Website\": s[13], \"Country\": s[14], \n",
    "               \"Acrivity\": int(s[15]), \"Followers\": int(s[16]), \n",
    "               \"Following\": int(s[17]) , \"Location\": s[18]}]\n",
    "  except Exception as err:\n",
    "      print(\"Wrong line format (%s): \" % line)\n",
    "      return []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lectura de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kafka: Lectura de datos\n",
    "# python practica_kafka_producer.py 0.4 0.9 Quatar_GP_2014 data/DATASET-Twitter-23-26-Mar-2014-MotoGP-Qatar.csv\n",
    "kafkaBrokerIPPort = \"127.0.0.1:9092\"\n",
    "\n",
    "kafkaParams = {\"metadata.broker.list\": kafkaBrokerIPPort}\n",
    "stream = KafkaUtils.createDirectStream(ssc, [\"Quatar_GP_2014\"], kafkaParams)\n",
    "stream = stream.map(lambda o: str(o[1])) # lo que devuelve es un dstream"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a) Calcular el número total de menciones recibidas por cada cuenta de usuario durante el intervalo de 5 segundos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet = stream.flatMap(parse_tweet)\n",
    "\n",
    "numMenUser = tweet.flatMap(lambda o : (o[\"Mentions\"].split(\",\")))\\\n",
    "                  .map(lambda o : o.strip('\"'))\\\n",
    "                  .map(lambda o : (o, 1))\\\n",
    "                  .reduceByKey(add) \n",
    "\n",
    "numMenUser.pprint()\n",
    "\n",
    "numMenUser.repartition(1).saveAsTextFiles(\"data/output_a/metrics\", \"txt\") # "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) Calcular la frecuencia total acumulada de apariciones de cada hashtag en el campo body, actualizando un ranking con los 5 hashtags con mayor frecuencia de aparición."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet = stream.flatMap(parse_tweet)\n",
    "\n",
    "numHash = tweet.filter(lambda o : o != \"\")\\\n",
    "               .flatMap(lambda o : (o[\"Body\"].split()))\\\n",
    "               .map(lambda o : o.strip('\"'))\\\n",
    "               .filter(lambda o : o[0] == \"#\")\\\n",
    "               .map(lambda o : (o, 1))\\\n",
    "               .reduceByKey(add) \n",
    "\n",
    "acumHash = numHash.updateStateByKey(lambda vals , totalHash : sum(vals) + totalHash if totalHash != None else sum(vals)) \n",
    "top5hash = acumHash.transform(lambda rdd: rdd.sortBy(lambda x: x[1], False).map(lambda x: x[0]).zipWithIndex().filter(lambda x: x[1] < 5)) \n",
    "\n",
    "sc.setCheckpointDir(\"data/checkpoint_b/\")\n",
    "top5hash.pprint()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c) Calcular en una ventana temporal 20 segundos con offset de 10 segundos la frecuencia de aparición de cada uno de los 3 posibles tipos de tweets (TW-RT-MT)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet = stream.flatMap(parse_tweet)\n",
    "\n",
    "tweetPerWindow = tweet.map(lambda o: (o['Tipe_action'],1))\\\n",
    "                      .reduceByKeyAndWindow(add, sub, 20, 10) \n",
    "\n",
    "sc.setCheckpointDir(\"data/checkpoint_c/\")\n",
    "\n",
    "tweetPerWindow.pprint()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start Streaming context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssc.start()\n",
    "ssc.awaitTerminationOrTimeout(30)  # Espera 30 segs. antes de acabar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssc.stop(False)  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
